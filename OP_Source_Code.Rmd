

---
title: "An Analytical Framework for Measuring Inequalities in the Public Opinions on Policing"
subtitle: "Assessing the Impacts of COVID-19 Pandemic in using Twitter Data"

author: |
  | `Authors:`
  | `Adepeju, M* and Jimoh, F**`
  | `*Big Data Centre, Manchester Metropolitan University, Manchester, M15 6BH`
  | `**School of Science, Engineering and Environment, University of Salford, United Kingdom`
  | `Contact: m.adepeju@mmu.ac.uk`

date: |
  | `Date:`
  | ``r Sys.Date()``
output:
  rmarkdown::html_vignette

#dev: png
#output:
  #word_document: default
  #always_allow_html: yes
#  pdf_document: default
always_allow_html: yes
#fig_caption: yes

---

<style type="text/css">

h1.title {
  font-size: 26px;
  line-height: 130%;
  color: Black;
  text-align: center;
}

<!-- h2.subtitle { -->
<!--   font-size: 26px; -->
<!--   line-height: 130%; -->
<!--   color: Red; -->
<!--   text-align: center; -->
<!-- } -->

h2.subtitle {
  text-align: center;
}

h4.author { /* Header 4 - and the author and data headers use this too  */
  font-size: 17px;
  font-family: "Arial";
  color: Black;
  text-align: center;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 17px;
  font-family: "Arial", Times, serif;
  color: Black;
  text-align: center;
}

h4.abstract { /* Header 4 - and the author and data headers use this too  */
  font-size: 10px;
  font-family: "Arial", Times, serif;
  color: black;
  text-align: center;
}

h4.institute{ /* Header 4 - and the author and data headers use this too  */
  font-size: 10px;
  font-family: "Arial", Times, serif;
  color: black;
  text-align: center;
}

body, td {
   font-size: 14px;
}
code.r{
  font-size: 13px;
}
pre {
  font-size: 13px
}
h1 { /* Header 1 */
  font-size: 16px;
  color: DarkBlue;
}
h2 { /* Header 2 */
    font-size: 16px;
  color: DarkBlue;

}
h3 { /* Header 3 */
  font-size: 15px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;

</style>

<!-- --- -->
<!-- title: "An Analytical Framework for Measuring Inequalities in the Public Opinions on Policing – " -->
<!-- subtitle: "Assessing COVID-19 Pandemic in Space-Time using Twitter Data" -->
<!-- author: "Monsuru Adepeju" -->
<!-- date: '`r format(Sys.time(), "%d %B, %Y (%H:%M)")`' -->
<!-- output: -->
<!--   html_document:  -->
<!--     toc: yes -->
<!--     pandoc_args: [ -->
<!--       "+RTS", "-K64m", -->
<!--       "-RTS" -->
<!--     ] -->
<!--   pdf_document: -->
<!--     fig_crop: no -->
<!--     highlight: kate -->
<!--     # keep_tex: yes -->
<!--     # latex_engine: xelatex -->
<!--     number_sections: yes -->
<!-- fontsize: 10pt -->
<!-- --- -->

```{r functions, include=FALSE}
# A function for captioning and referencing images
fig <- local({
    i <- 0
    ref <- list()
    list(
        cap=function(refName, text) {
            i <<- i + 1
            ref[[refName]] <<- i
            paste("Figure ", i, ": ", text, sep="")
        },
        ref=function(refName) {
            ref[[refName]]
        })
})
```


# Preface

_This is the accompanying source code for the paper 'An Analytical Framework for Measuring Inequalities in the Public Opinions on Policing – Assessing COVID-19 Pandemic in Space-Time using Twitter Data', submitted to JGIS_

# Introduction

The paper (mentioned above) proposes an analytical framework for measuring the inequalities in the public opinion on policing. The aim of this document is to allow users to replicate the outputs as presented in the paper using their own datasets. The document is divided into different sections which complete different component of the analysis.

## Configuration and library loading

Configure the script here. As well as making sure all the libraries used below have been installed, you need to:

 1. Set your working directory (the `WORKING_DIR` variable below) to be the location of this script.
 2. Install all the libraries required to complete the analysis.
 

```{r initialise, message=FALSE}

# This directory needs to be set to the location of this script on your PC
WORKING_DIR <- 'C:/R/Github/JGIS_Policing_COVID-19'

#setting working directory
setwd(WORKING_DIR)

library(twitteR) #for setting up Twitter authorization
library(rtweet) #for creating Twitter Authorization Token(S).
library(dplyr) #for data manipulation and analysis
library(colormap)
library(fmsb)
library(tidyr)
library(radarchart)
library(grDevices)
library(plotly)
library(webshot)
library(rgdal)
library(sf)
library(stringr)
library(tidytext)
library(reshape2)
library(stringdist)
library(tidyverse)
library(lubridate)
library(scales)
library(ggradar)
library(RColorBrewer)
library(ggpubr)
library(see)
library(likert)
library(rgdal) 
library(rgeos)
library(maptools)
library(ggplot2)
library(scales) 

```
 
# Downloading Twitter Data

All new developers must apply for a developer account to access Twitter APIs (see [here](https://developer.twitter.com/en/apply-for-access)). Once approved, you follow the instruction on this [page](https://developer.twitter.com/en/docs/twitter-api/getting-started/guide). Within the developer App, you will be provided a set of API Keys (also known as Consumer Keys). You will also have the chance to generate a set of Access Tokens that can be used to make tweet requests, and a Bearer Token that can be used to authenticate endpoints. Below, we demonstrate how we used one of the Author's keys and token to download tweets within a defined geographical coverage. The tweets needed for this analysis are tweets that include the keywords or hashtages: 'police', 'policing', and 'law enforcement(s)' (Note: the `eval=FALSE` argument below is to skip the chunk of the script). Modify the chunk as appropriate.


```{r, message=FALSE, eval=FALSE}

#define keys and tokens (Note: real keys or tokens do not contain asterisk as below)
consumer_key <- 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX' 
consumer_secret <- 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'
access_token <- 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'
access_secret <- 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)

token <- create_token(
  app = "UK2019", #App name
  consumer_key = consumer_key,
  consumer_secret = consumer_secret)

#define the keywords to search in tweets
keywords <- c("police", "policing", "law enforcement", "law enforcements")

#tweets holder
all_Tweets <- NULL

#Given a party  
for(i in seq_len(length(keywords))){ #i<-1
  
  tweets_g1 <- NULL
  #tweets_g2 <- NULL
  
  tweets_g1 <- search_tweets(q=keywords[i],  n=17500, type="recent", include_rts=TRUE, 
                             token = token, lang="en",geocode='53.805,-4.242,350mi') 
  #the lat, long and rad above defines the geographical coverage within which the download is performed.
  if(nrow(tweets_g1)!=0){
    tweets_g1 <- tweets_g1 %>% dplyr::mutate(class=keywords[i])
    all_Tweets <- rbind(all_Tweets, tweets_g1)  #all_Tweets<-NULL
  }

  flush.console()
  print(paste(nrow(tweets_g1), nrow(tweets_g1), sep="||"))
  print("waiting for 15.5 minutes")
  testit(960) #wait for another 15 minutes before downloading again
}

#save the results
write_as_csv(all_Tweets, paste("/data/only_policeTweet_set_", 1, "_.csv", sep=""), na="NA", fileEncoding = "UTF-8")
#change the value "1" for subsequent downloading.

#After downloading, add files inside the `data` directories.

```

# Geocoding

The downloaded tweet files are read into RStudio and appended. Next, we geocode each tweets to its respective police force areas (PFA) using the [`PFA-Location lookup`](./data/PoliceForce_location.csv) table. Import other necessary files.

```{r, eval = FALSE}
#read in downloaded tweets#
#Note: we do not add these datasets due to restriction place by the `Twitter's Developer Agreement and Policy`. The Policy can be found here: "https://developer.twitter.com/en/developer-terms/agreement-and-policy".

data_sample1 <- read.table(file="/data/only_policeTweet_set_1.csv", sep=",", head=TRUE)
.
.
data_sampleN <- read.table(file="/data/only_policeTweet_set_N.csv", sep=",", head=TRUE)

#combine data
data <- rbind(data_sample1, ....., data_sampleN)

#format date field
data$created_at <- as.Date(data$created_at)

#create and append time steps (or periods)
data_ <- data %>% 
  dplyr::mutate(period = if_else((created_at >= "2020-10-20" & created_at <= "2020-11-19"), "Period1",
                                 if_else(created_at >= "2020-11-20" & created_at <= "2020-12-19", "Period2",
                                         if_else(created_at >= "2020-12-20" & created_at <= "2021-01-19", "Period3", "0"))))             
#house cleaning
rm(list=ls()[! ls() %in% 
  c("data_")])

#read in the PFA-location lookup table
location <- read.csv(file="/data/PoliceForce_location.csv", sep=",", head=TRUE)

#preview
head(location)
#check length
nrow(location)

#summary of location table
table(location$policeForce)

#Subset only the tweets needed (i.e. Organic tweets and Replies)
data1 = data_ %>%
  dplyr::arrange(status_id) %>%
  dplyr::filter(!duplicated(status_id))%>%
  dplyr::filter(is_retweet==FALSE)%>% #remove retweets
  dplyr::mutate(location1= gsub(",.*$", "", location)) %>% #add city name
  dplyr::select(-c(location))%>%
  dplyr::rename(location=location1)

head(data1)

#geocoding
data_1 <- data1 %>% 
  inner_join(location)

#house cleaning
rm(list=ls()[! ls() %in% 
               c("data_1")])

#get names of policing regions
Pf_names_regions <- read.table(file="/data/Regions.csv", sep=",", head=TRUE)

#collate names of PFAs
Pf_names_regions_uni <- Pf_names_regions$Police.Force

#house cleaning
rm(list=ls()[! ls() %in% 
               c("data_1", "Pf_names_regions_uni", "fix.contractions", "removeSpecialChars")])

#get the time steps
periods <- unique(data_1$period)[2:4]

```


# Data Cleaning, Tidying and Sentiment Analysis

```{r, eval=FALSE}

#given a police force area
#--------------------------------
for(i in 1:length(Pf_names_regions_uni)){ #loop through policing regions

  tweet_container_afinn <- NULL
  
  #given a time step (period)
  for(z in 1:length(periods)){ #z = 1
    
    #use sample of London tweets (25%)
    if(Pf_names_regions_uni[i]=="Metropolitan Police"){
      #clean tweets
      placeTwt <- data_1 %>% 
        dplyr::filter(period == periods[z])%>%
        dplyr::filter(policeForce==Pf_names_regions_uni[i]) %>%
        dplyr::select(text) %>%
        dplyr::mutate(text = gsub("http://*|https://*|https*|\n*|*>|<*","", text)) %>%
        mutate(text=str_replace_all(text, "[[:punct:]]", " "))
        set.seed(1000)
        placeTwt <- sample_frac(placeTwt, 0.25)
    }
    
    if(Pf_names_regions_uni[i]!="Metropolitan Police"){
      #clean tweets
      placeTwt <- data_1 %>% 
        dplyr::filter(period == periods[z])%>%
        dplyr::filter(policeForce==Pf_names_regions_uni[i]) %>%
        dplyr::select(text) %>%
        dplyr::mutate(text = gsub("http://*|https://*|https*|\n*|*>|<*","", text)) %>%
        mutate(text=str_replace_all(text, "[[:punct:]]", " "))
    }

    #more cleaning
    placeTwt <- unlist(placeTwt) %>% stringr::str_remove(pattern = "t co.*")
    
    #to detect the similarities
    placeTwt <- data.frame(cbind(text1=placeTwt, text2=c("NULL", placeTwt[1:length(placeTwt)-1])))
    dim(placeTwt)
    head(placeTwt)
    
    #filter duplicates based on the similarities index
    similarities <- stringsim(placeTwt$text1,placeTwt$text2,method='lcs', p=0.25)
    
    placeTwt <- placeTwt[, 1]
    dim(placeTwt) #head(placeTwt)
    placeTwt <- placeTwt[which(similarities!=1)]
    
    placeTwt <- data.frame(text=placeTwt)
    
    #remove emoticons
    placeTwt$text <- sapply(placeTwt,function(row) iconv(row, "latin1", "ASCII", sub=""))
    
    placeTwt$text <- sapply(placeTwt , fix.contractions)
    
    #remove special characters
    placeTwt$text <- sapply(placeTwt$text, removeSpecialChars)
    
    #convert all text to lower case
    placeTwt$text <- sapply(placeTwt$text, tolower)
    
    placeTwt$text <- sapply(placeTwt,function(row) iconv(row, "latin1", "ASCII", sub=""))
    
    #detect tweets with pandemic keywords
    placeTwt_keyword_exist <- data.frame(placeTwt$text) %>%
    dplyr::filter(stringr::str_detect(text,'pandemic|pandemics|lockdown|lockdowns|corona|coronavirus|covid|covid19|covid-19|virus|viruses|quarantine|infect|infects|infecting|infected', negate=FALSE))
    #detect tweets with no pandemic keywords
    placeTwt_no_keyword <- data.frame(placeTwt$text) %>%
      dplyr::filter(stringr::str_detect(text, 'pandemic|pandemics|lockdown|lockdowns|corona|coronavirus|covid|covid19|covid-19|virus|viruses|quarantine|infect|infects|infecting|infected', negate=TRUE))
    
    #create the id field and drop text field
    placeTwt_keyword_exist$ID <- seq.int(nrow(placeTwt_keyword_exist)) 
    
    #do sentiment analysis and also detect negated sentiment for 
    #tweets with pandemic keywords
    token_neg_exist <- as_tibble(placeTwt_keyword_exist) %>%
      #unnest_tokens(word, text)%>%
      unnest_tokens(bigram, text, token = "ngrams", n = 2)%>%
      separate(bigram, c("word1", "word2"), sep = " ")%>%
      as.data.frame() %>%
      #filter the preceeding by the 'negation' word
      dplyr::filter(word1 %in% c("not", "never", "no", "without")) %>%
      dplyr::mutate(neg = paste(word1, word2, sep=" ")) %>% #get sentiment score
      dplyr::rename(word  = word2)%>%
      left_join(get_sentiments("afinn")) %>% #
      dplyr::select(ID, neg, value)%>%
      dplyr::mutate(value2 = value * 2)%>%#reverse the score (and multiply by 2)
      dplyr::mutate(value = value2)%>%
      dplyr::select(-c(value2))%>%
      dplyr::rename(word=neg)%>%
      dplyr::filter(!is.na(value))%>%
      mutate(pandem_keyword="exist")%>%
      mutate(country=Pf_names_regions_uni[i])%>%
      mutate(Period = periods[z])
    
    placeTwt_all_DATA_afinn_exist <- placeTwt_keyword_exist %>%
      unnest_tokens(word, text, drop = FALSE) %>%
      inner_join(get_sentiments("afinn")) %>% #join with the lexicon
      mutate(pandem_keyword="exist")%>%
      mutate(country=Pf_names_regions_uni[i])%>%
      mutate(Period = periods[z])%>% 
      dplyr::select(-c(text))#drop text
    
    #create the id field and drop text field
    placeTwt_all_DATA_afinn_exist <- data.frame(rbind(placeTwt_all_DATA_afinn_exist,
                                                      token_neg_exist))
  
    #create the sentiment data (bing)
    placeTwt_no_keyword$ID <- seq.int(nrow(placeTwt_no_keyword)) #create an id field
    #get an ngram (2-ngrame)
    
    #do sentiment analysis and also detect negated sentiment for 
    #tweets with no pandemic keywords
    token_neg_no_keyword <- as_tibble(placeTwt_no_keyword) %>%
      #unnest_tokens(word, text)%>%
      unnest_tokens(bigram, text, token = "ngrams", n = 2)%>%
      separate(bigram, c("word1", "word2"), sep = " ")%>%
      as.data.frame() %>%
      #filter the preceeding by the 'negation' word
      dplyr::filter(word1 %in% c("not", "never", "no", "without")) %>%
      dplyr::mutate(neg = paste(word1, word2, sep=" ")) %>% #get sentiment score
      dplyr::rename(word  = word2)%>%
      left_join(get_sentiments("afinn")) %>% #
      dplyr::select(ID, neg, value)%>%
      dplyr::mutate(value2 = value * 2)%>%#reverse the score (and multiply by 2)
      dplyr::mutate(value = value2)%>%
      dplyr::select(-c(value2))%>%
      dplyr::rename(word=neg)%>%
      dplyr::filter(!is.na(value))%>%
      mutate(pandem_keyword="exist")%>%
      mutate(country=Pf_names_regions_uni[i])%>%
      mutate(Period = periods[z])
    
    placeTwt_all_DATA_afinn_absent <- placeTwt_no_keyword %>%
      unnest_tokens(word, text, drop = FALSE) %>%
      inner_join(get_sentiments("afinn")) %>% #join with the lexicon
      mutate(pandem_keyword="absent")%>%
      mutate(country=Pf_names_regions_uni[i])%>%
      mutate(Period = periods[z])%>%
      dplyr::select(-c(text))
    
    #join the two observed sentiment document (OSD) documents
    placeTwt_all_DATA_afinn_absent <- data.frame(rbind(placeTwt_all_DATA_afinn_absent,
                                                       token_neg_no_keyword))
    
    placeTwt_all_DATA_afinn_observed <- data.frame(rbind(placeTwt_all_DATA_afinn_exist, placeTwt_all_DATA_afinn_absent))
    
    tweet_container_afinn <- rbind(tweet_container_afinn, placeTwt_all_DATA_afinn_observed)
    
  }
  
  flush.console()
  print(paste(i, z, " | "))
  
  #write the OSD for the current PFA into the `output` directory
  write.table(tweet_container_afinn, file = paste("/outputs/", 
                                                  "police_cleaned_", Pf_names_regions_uni[i], "_all_DATA_afinn_observed_ts", 
                                                  ".csv", sep=""), sep=",", row.names = F)
  
}

```


# Compute the Opinion Scores and p-values

This chunk computes:

i. the opinion score of each tweet, 
ii. then resultant opinion scores of each PFA
iii. Derive the expected sentiment document (ESD)
iv. Do randomization testing and compute the p-values

The key outputs of this chunk are the:

a. `Observation` tables showing the computed OP scores across PFAs and time steps, 
b.  `P-value` tables showing the statistical significant values based on N replications, and the 
c. `Position` table that describe the position of an observed OP score relative to the mean expectation.

```{r, eval=FALSE}

#names of policing region
regions <- c("North West", "North East","Yorkshire and the Humber",
             "West Midlands","East Midlands","Eastern",
             "Wales", "South West","South East")

#start
t1 <- Sys.time()

op <- par(mfrow = c(2,2),
          oma = c(5,4,0,0) + 0.5,
          mar = c(0,0,4,4) + 0.5)


#all_UK_bing_combined <- NULL
all_Regional_afinn_combined <- NULL
all_pvalueS_1 <- NULL
all_pvalueS_2 <- NULL
all_pvalueS_3 <- NULL

all_p_signs_1 <- matrix(0, 42, 3)
all_p_signs_2 <- matrix(0, 42, 3)
all_p_signs_3 <- matrix(0, 42, 3)

init <- 0
#generate the expected and observed for all regions

#loop throug region
for(i in 1:length(regions)){ #i=1

  subsetP <- Pf_names_regions %>% 
    filter(Regions == regions[i])
  
  Region <- regions[i]
  
  #UK_bing_combined <- NULL
  Regional_afinn_combined <- NULL
  
  for(j in 1:nrow(subsetP)) {#j=1
    
    init <- init + 1
    
    #read in the OSD
    UK_afinn_read <- read.table(file = paste("/outputs/", 
                                             "police_cleaned_", subsetP$Police.Force[j],"_all_DATA_afinn_observed_ts", 
                                             ".csv", sep=""), sep=",", head=TRUE)
    
    #compute a unique sentiment for each tweet
    UK_afinn_read <- UK_afinn_read %>% 
      dplyr::group_by(Period, country, pandem_keyword, ID)%>%
      dplyr::summarise(sentiment_score = sum(value))%>%
      dplyr::filter(sentiment_score != 0) %>%
      dplyr::mutate(sentiment = if_else(sentiment_score > 0, "positive", "negative"))%>%
      dplyr::select(-c(sentiment_score))%>%
      ungroup %>%
      dplyr::mutate(word = "word")%>%
      dplyr::select(ID, word, sentiment, pandem_keyword,country,Period)
    
    head(UK_afinn_read)
    
    unique(UK_afinn_read$pandem_keyword)
    unique(UK_afinn_read$Period)
    #word sentiment pandem_keyword  country
  
    #create Expected Ssentiment Document
    #compute the probability of observed from the 'absent' group
    #generate the sentiment list based on the prob.
    #assign the 'exist' group with new sentiments based on the prob
    #then plot the expected and observed.
    
    periods <- unique(UK_afinn_read$Period)
    
    container2_holder <- NULL
    
    for(y in 1:length(periods)){  #y<-1
      
      UK_afinn_read_subset <- UK_afinn_read %>%
        dplyr::filter(Period == periods[y])
      
      #--------------------------------------
      #randomization testing (replicas can be reduced to e.g. 99 for runtime)
      esd_simulated <- net_sentiment_simulation(data = UK_afinn_read_subset, replicas=999)
      
      
      #compute the observed from the original data
      UK_afinn_OSD <- UK_afinn_read_subset %>% 
        dplyr::select(ID, word, sentiment, pandem_keyword, country)

      UK_afinn_OSD <-  UK_afinn_OSD %>%
        #filter(country != "NA" & !sentiment %in% c("positive", "negative")) %>%
        count(sentiment, country) %>%
        group_by(country, sentiment) %>%
        summarise(sentiment_sum = sum(n)) %>% #here is where to deduct the negation
        ungroup()
      
      #calculate % sentiment
      UK_afinn_OSD_ = UK_afinn_OSD %>% 
        group_by(country) %>%
        dplyr::mutate(total=sum(sentiment_sum))%>%
        mutate(pct=round((sentiment_sum/total)*100, digits=2))
      
      UK_afinn_OSD_ = data.frame(dcast(UK_afinn_OSD_, sentiment ~ country))
      
      UK_afinn_2_OSD = UK_afinn_OSD_ %>% gather(Country, valname, -sentiment) %>% 
        spread(sentiment, valname)%>%
        dplyr::rename(negative_OSD = negative)%>%
        dplyr::rename(positive_OSD=positive) %>%
        mutate(net_sentiment_OSD =  positive_OSD -  negative_OSD)
      
      #contruct the confidence interval (ave +/- z * se) and compute p-value
      if(y == 1){
        if(UK_afinn_2_OSD$net_sentiment_OSD > mean(esd_simulated$net_sentiment_ESD)){
          all_p_signs_1[init, 1] <- "TRUE"
        }
        all_p_signs_1[init, 2] <- subsetP$Police.Force[j]
        all_p_signs_1[init, 3] <- regions[i]
      }
      
      if(y == 2){
        if(UK_afinn_2_OSD$net_sentiment_OSD > mean(esd_simulated$net_sentiment_ESD)){
          all_p_signs_2[init, 1] <- "TRUE"
        }
        all_p_signs_2[init, 2] <- subsetP$Police.Force[j]
        all_p_signs_2[init, 3] <- regions[i]
      }
      
      if(y == 3){
        if(UK_afinn_2_OSD$net_sentiment_OSD > mean(esd_simulated$net_sentiment_ESD)){
          all_p_signs_3[init, 1] <- "TRUE"
        }
        all_p_signs_3[init, 2] <- subsetP$Police.Force[j]
        all_p_signs_3[init, 3] <- regions[i]
      }
      
      beat_left <- length(which(esd_simulated$net_sentiment_ESD >  UK_afinn_2_OSD$net_sentiment_OSD))
      beat_right <- length(which(esd_simulated$net_sentiment_ESD <  UK_afinn_2_OSD$net_sentiment_OSD))
      Sbeat <- min(c(beat_left, beat_right))
      pvalue_sentiment <- (Sbeat + 1)/(length(esd_simulated$net_sentiment_ESD) + 1)
      
      if(y == 1){
        all_pvalueS_1 <- rbind(all_pvalueS_1, cbind(pvalue_sentiment, subsetP$Police.Force[j], regions[i]))
      }
      
      if(y == 2){
        all_pvalueS_2 <- rbind(all_pvalueS_2, cbind(pvalue_sentiment, subsetP$Police.Force[j], regions[i]))
      }
      
      if(y == 3){
        all_pvalueS_3 <- rbind(all_pvalueS_3, cbind(pvalue_sentiment, subsetP$Police.Force[j], regions[i]))
      }

      mean_exp <- data.frame(matrix(colMeans(esd_simulated[,2:ncol(esd_simulated)]),1,))
      colnames(mean_exp)<-colnames(esd_simulated)[2:length(colnames(esd_simulated))]
      
      #combine periods data
      container2_holder <- rbind(container2_holder, 
                                 cbind(UK_afinn_2_OSD, mean_exp)%>% mutate(Period=periods[y]))
    }
    
    #combine the regional data
    Regional_afinn_combined <- rbind(Regional_afinn_combined, container2_holder)
    
    
  }
  
  all_Regional_afinn_combined <- rbind(all_Regional_afinn_combined, Regional_afinn_combined %>%
                                         mutate(Region = regions[i]))
  
  flush.console()
  print(i)
}

#end time
t2 <- Sys.time()

#run time
t_diff <- t2 -t1

#Previewing the results
#read.table

#Export the 'Observation' table (Opinion scores for time steps 1, 2, 3, called 'Periods')
write.table(all_Regional_afinn_combined, file="/outputs/all_obs_exp_Sentiment_ts_1_2_3_ALL.csv", sep=",", row.names = F)

#P-values and position table for time step 1
write.table(all_pvalueS_1, file="/outputs/all_pvalue_Sentiment_ts_1.csv", sep=",", row.names = F)
write.table(all_p_signs_1, file="/outputs/all_sentiment_signs_ts_1.csv", sep=",", row.names = F)

#P-values and position table for time step 2
write.table(all_pvalueS_2, file="/outputs/all_pvalue_Sentiment_ts_2.csv", sep=",", row.names = F)
write.table(all_p_signs_2, file="/outputs/all_sentiment_signs_ts_2.csv", sep=",", row.names = F)

#P-values and position table for time step 3
write.table(all_pvalueS_3, file="/outputs/all_pvalue_Sentiment_ts_3.csv", sep=",", row.names = F)
write.table(all_p_signs_3, file="/outputs/all_sentiment_signs_ts_3.csv", sep=",", row.names = F)

```


# Plotting

In this section, we provide codes for visualizing the results based on the tables generated above.

### Radar chart

To plot the measured opinion across the police force areas (PFAs)

```{r, eval=FALSE}

#define names and order of regions
regions <- c("North West", "North East","Yorkshire and the Humber",
             "West Midlands","East Midlands","Eastern",
             "Wales", "South West","South East")


#read in observation table
all_UK_bing_combined <- read.table(file="/outputs/all_obs_exp_Sentiment_ts_1_2_3_ALL_6.csv", sep=",", head=TRUE)

#read in 'Position' tables
all_pvalueS_1 <- read.table(file="/outputs/all_pvalue_Sentiment_ts_1.csv", sep=",", head=TRUE)
all_pvalueS_2 <- read.table(file="/outputs/all_pvalue_Sentiment_ts_2.csv", sep=",", head=TRUE)
all_pvalueS_3 <- read.table(file="/outputs/all_pvalue_Sentiment_ts_3.csv", sep=",", head=TRUE)

#read in P-values
all_p_signs_1 <- read.table(file="/outputs/all_sentiment_signs_ts_1.csv", sep=",", head=TRUE)
all_p_signs_2 <- read.table(file="/outputs/all_sentiment_signs_ts_2.csv", sep=",", head=TRUE)
all_p_signs_3 <- read.table(file="/outputs/all_sentiment_signs_ts_3.csv", sep=",", head=TRUE)


for(k in 1:length(regions)){ 
  
  
  UK_bing_combined_subset <- all_UK_bing_combined[which(all_UK_bing_combined$Region %in% regions[k]),] %>%
    select(-c(Region))
  
  UK_bing_combined_subset <- UK_bing_combined_subset %>%
    dplyr::select(c(Country, Period, net_sentiment_OSD))
  
  UK_bing_combined_subset <- t(dcast(UK_bing_combined_subset, Period ~ Country))
  
  UK_bing_combined_subset <- UK_bing_combined_subset[2:nrow(UK_bing_combined_subset), ]
  
  UK_bing_combined_subset <- data.frame(cbind(rownames(UK_bing_combined_subset), UK_bing_combined_subset))
  
  colnames(UK_bing_combined_subset) <- c("Group", "Period1","Period2","Period3")
  
  UK_bing_combined_subset$Period1 <- as.numeric(UK_bing_combined_subset$Period1)
  UK_bing_combined_subset$Period2 <- as.numeric(UK_bing_combined_subset$Period2)
  UK_bing_combined_subset$Period3 <- as.numeric(UK_bing_combined_subset$Period3)
  
  
  df.UK_bing_combined_subset_2 <- melt(UK_bing_combined_subset, 
                                       id.vars = c("Group"), 
                                       measure.vars = colnames(UK_bing_combined_subset)[2:length(colnames(UK_bing_combined_subset))],
                                       variable.name = "Key",
                                       value.name = "Score")
  
  col = c(viridis(10)[9], viridis(10)[6], viridis(10)[3])
  linetp <- c(rep(1, nrow(df.UK_bing_combined_subset_2)/3),
              rep(2, nrow(df.UK_bing_combined_subset_2)/3), 
              rep(3, nrow(df.UK_bing_combined_subset_2)/3))
  
  if(regions[k]=="North West"){
    df.UK_bing_combined_subset_2 <- df.UK_bing_combined_subset_2 %>%
      #group_by(Key)%>%
      arrange(desc(Key), Group)%>%
      as.data.frame()
    
    North_West = df.UK_bing_combined_subset_2 %>%
      ggplot(aes(x = Group,y = Score,color = Key,group = Key,fill = Key)) +
      geom_polygon(size = 1, alpha = .1, fill=NA,linetype=linetp) +
      scale_color_manual(values=col)+
      geom_point(size = 3) +
      ylim(min(df.UK_bing_combined_subset_2$Score)-5, max(df.UK_bing_combined_subset_2$Score)) + ggtitle(regions[k])  + 
      scale_x_discrete() +
      xlab("")+
      theme_light() +
      #scale_color_manual(values = c("Red", "Blue")) +
      #scale_fill_manual(values = c("Red", "Blue")) +
      coord_radar() #+
    #theme_radar()
  }
  
  North_West
  
  if(regions[k]=="North East"){
    df.UK_bing_combined_subset_2 <- df.UK_bing_combined_subset_2 %>%
      #group_by(Key)%>%
      arrange(desc(Key), Group)%>%
      as.data.frame()
    
    North_East = df.UK_bing_combined_subset_2 %>%
      ggplot(aes(x = Group,y = Score,color = Key,group = Key,fill = Key)) +
      geom_polygon(size = 1, alpha = .1, fill=NA,linetype=linetp) +
      scale_color_manual(values=col)+
      geom_point(size = 3) +
      ylim(min(df.UK_bing_combined_subset_2$Score)-5, max(df.UK_bing_combined_subset_2$Score)) + ggtitle(regions[k])  + 
      scale_x_discrete() +
      xlab("")+
      theme_light() +
      #scale_color_manual(values = c("Red", "Blue")) +
      #scale_fill_manual(values = c("Red", "Blue")) +
      coord_radar() #+
    #theme_radar()
  }
  
  if(regions[k]=="Yorkshire and the Humber"){
    df.UK_bing_combined_subset_2 <- df.UK_bing_combined_subset_2 %>%
      #group_by(Key)%>%
      arrange(desc(Key), Group)%>%
      as.data.frame()
    
    Yorkshire_and_the_Humber = df.UK_bing_combined_subset_2 %>%
      ggplot(aes(x = Group,y = Score,color = Key,group = Key,fill = Key)) +
      geom_polygon(size = 1, alpha = .1, fill=NA,linetype=linetp) +
      scale_color_manual(values=col)+
      geom_point(size = 3) +
      ylim(min(df.UK_bing_combined_subset_2$Score)-5, max(df.UK_bing_combined_subset_2$Score)) + ggtitle(regions[k])  + 
      scale_x_discrete() +
      xlab("")+
      theme_light() +
      #scale_color_manual(values = c("Red", "Blue")) +
      #scale_fill_manual(values = c("Red", "Blue")) +
      coord_radar() #+
    #theme_radar()
  }
  
  if(regions[k]=="West Midlands"){
    df.UK_bing_combined_subset_2 <- df.UK_bing_combined_subset_2 %>%
      #group_by(Key)%>%
      arrange(desc(Key), Group)%>%
      as.data.frame()
    
    West_Midlands = df.UK_bing_combined_subset_2 %>%
      ggplot(aes(x = Group,y = Score,color = Key,group = Key,fill = Key)) +
      geom_polygon(size = 1, alpha = .1, fill=NA,linetype=linetp) +
      scale_color_manual(values=col)+
      geom_point(size = 3) +
      ylim(min(df.UK_bing_combined_subset_2$Score)-5, max(df.UK_bing_combined_subset_2$Score)) + ggtitle(regions[k])  + 
      scale_x_discrete() +
      xlab("")+
      theme_light() +
      #scale_color_manual(values = c("Red", "Blue")) +
      #scale_fill_manual(values = c("Red", "Blue")) +
      coord_radar() #+
    #theme_radar() 
  }
  
  max(df.UK_bing_combined_subset_2$Score)  
  
  if(regions[k]=="East Midlands"){
    df.UK_bing_combined_subset_2 <- df.UK_bing_combined_subset_2 %>%
      #group_by(Key)%>%
      arrange(desc(Key), Group)%>%
      as.data.frame()
    
    East_Midlands = df.UK_bing_combined_subset_2 %>%
      ggplot(aes(x = Group,y = Score,color = Key,group = Key,fill = Key)) +
      geom_polygon(size = 1, alpha = .1, fill=NA,linetype=linetp) +
      scale_color_manual(values=col)+
      geom_point(size = 3) +
      ylim(min(df.UK_bing_combined_subset_2$Score)-5, max(df.UK_bing_combined_subset_2$Score)) + ggtitle(regions[k])  + 
      scale_x_discrete() +
      xlab("")+
      theme_light() +
      #scale_color_manual(values = c("Red", "Blue")) +
      #scale_fill_manual(values = c("Red", "Blue")) +
      coord_radar() #+
    #theme_radar()
  }
  
  if(regions[k]=="Eastern"){
    df.UK_bing_combined_subset_2 <- df.UK_bing_combined_subset_2 %>%
      #group_by(Key)%>%
      arrange(desc(Key), Group)%>%
      as.data.frame()
    
    Eastern = df.UK_bing_combined_subset_2 %>%
      ggplot(aes(x = Group,y = Score,color = Key,group = Key,fill = Key)) +
      geom_polygon(size = 1, alpha = .1, fill=NA,linetype=linetp) +
      scale_color_manual(values=col)+
      geom_point(size = 3) +
      ylim(min(df.UK_bing_combined_subset_2$Score)-5, max(df.UK_bing_combined_subset_2$Score)) + ggtitle(regions[k])  + 
      scale_x_discrete() +
      xlab("")+
      theme_light() +
      #scale_color_manual(values = c("Red", "Blue")) +
      #scale_fill_manual(values = c("Red", "Blue")) +
      coord_radar() #+
    #theme_radar()
  }
  
  if(regions[k]=="Wales"){
    df.UK_bing_combined_subset_2 <- df.UK_bing_combined_subset_2 %>%
      #group_by(Key)%>%
      arrange(desc(Key), Group)%>%
      as.data.frame()
    
    Wales = df.UK_bing_combined_subset_2 %>%
      ggplot(aes(x = Group,y = Score,color = Key,group = Key,fill = Key)) +
      geom_polygon(size = 1, alpha = .1, fill=NA,linetype=linetp) +
      scale_color_manual(values=col)+
      geom_point(size = 3) +
      ylim(min(df.UK_bing_combined_subset_2$Score)-5, max(df.UK_bing_combined_subset_2$Score)) + ggtitle(regions[k])  + 
      scale_x_discrete() +
      xlab("")+
      theme_light() +
      #scale_color_manual(values = c("Red", "Blue")) +
      #scale_fill_manual(values = c("Red", "Blue")) +
      coord_radar() #+
    #theme_radar()
  }
  
  if(regions[k]=="South West"){
    df.UK_bing_combined_subset_2 <- df.UK_bing_combined_subset_2 %>%
      #group_by(Key)%>%
      arrange(desc(Key), Group)%>%
      as.data.frame()
    
    South_West = df.UK_bing_combined_subset_2 %>%
      ggplot(aes(x = Group,y = Score,color = Key,group = Key,fill = Key)) +
      geom_polygon(size = 1, alpha = .1, fill=NA,linetype=linetp) +
      scale_color_manual(values=col)+
      geom_point(size = 3) +
      ylim(min(df.UK_bing_combined_subset_2$Score)-5, max(df.UK_bing_combined_subset_2$Score)) + ggtitle(regions[k])  + 
      scale_x_discrete() +
      xlab("")+
      theme_light() +
      #scale_color_manual(values = c("Red", "Blue")) +
      #scale_fill_manual(values = c("Red", "Blue")) +
      coord_radar() #+
    #theme_radar()
  }
  
  if(regions[k]=="South East"){
    df.UK_bing_combined_subset_2 <- df.UK_bing_combined_subset_2 %>%
      #group_by(Key)%>%
      arrange(desc(Key), Group)%>%
      as.data.frame()
    
    South_East = df.UK_bing_combined_subset_2 %>%
      ggplot(aes(x = Group,y = Score,color = Key,group = Key,fill = Key)) +
      geom_polygon(size = 1, alpha = .1, fill=NA,linetype=linetp) +
      scale_color_manual(values=col)+
      geom_point(size = 3) +
      ylim(min(df.UK_bing_combined_subset_2$Score)-5, max(df.UK_bing_combined_subset_2$Score)) + ggtitle(regions[k])  + 
      scale_x_discrete() +
      xlab("")+
      theme_light() +
      #scale_color_manual(values = c("Red", "Blue")) +
      #scale_fill_manual(values = c("Red", "Blue")) +
      coord_radar() #+
    #theme_radar()
  }

}

```

```{r, eval=FALSE}
#visualization
ggarrange(
  North_West, North_East, Yorkshire_and_the_Humber, West_Midlands,
  common.legend = TRUE, nrow=2, ncol=2, legend = "bottom"
)
```

```{r figs1, echo=FALSE, eval=TRUE, include=TRUE, fig.cap=fig$cap("figs1", "Measured Opinions for regions 1 to 4"), out.width = '60%', fig.align="center"} 
knitr::include_graphics("figures/sent_1_4.png")
```

```{r, eval=FALSE}
ggarrange(
  East_Midlands,            
  Eastern, Wales, South_West, 
  common.legend = TRUE, nrow=2, ncol=2, legend = "bottom"
)
```

```{r figs2, echo=FALSE, eval=TRUE, include=TRUE, fig.cap=fig$cap("figs1", "Measured Opinions for regions 5 to 8"), out.width = '60%', fig.align="center"} 
knitr::include_graphics("figures/sent_5_8.png")
```

```{r, eval=FALSE}
ggarrange(
  South_East,
  common.legend = TRUE, nrow=2, ncol=2, legend = "bottom"
)
```

```{r figs3, echo=FALSE, eval=TRUE, include=TRUE, fig.cap=fig$cap("figs1", "Measured Opinions for region 9"), out.width = '60%', fig.align="center"} 
knitr::include_graphics("figures/sent_9_9.png")
```


### Likert Chart

To show the proportion of tweet types and sentiments per PFA

```{r, eval=FALSE}

Pf_names_regions <- read.table(file="Regions.csv", sep=",", head=TRUE)

# regions <- c("North West", "North East","Yorkshire and the Humber",
#              "West Midlands","East Midlands","Eastern",
#              "Wales", "South West","South East")


periods <- c("Period1", "Period2", "Period3")
#init <- 0

#generate the expected and observed for all regions
for(y in 1:length(periods)){  #y<-1
  
  likert_sentimentData <- NULL
  
  for(i in 1:length(unique(Pf_names_regions$Regions))){ #i=7
    
    subsetP <- Pf_names_regions %>% 
      filter(Regions == unique(Pf_names_regions$Regions)[i])
    
    #Region <- regions[i]
    
    for(j in 1:nrow(subsetP)) {#j=3
      #for(j in 1:3) {#j=1
      
      #init <- init + 1
      
      UK_afinn_read <- read.table(file = paste("/outputs/", "police_cleaned_", 
                                               subsetP$Police.Force[j],"_all_DATA_afinn_observed_ts", ".csv", sep=""), 
                                                  sep=",", head=TRUE)
      

      head(UK_afinn_read)
      UK_afinn_read[1:10,]
      
      unique(UK_afinn_read$Period)
      
      #compute a unique sentiment for each tweet
      UK_afinn_read <- UK_afinn_read %>% 
        dplyr::group_by(Period, country, pandem_keyword, ID)%>%
        dplyr::summarise(sentiment_score = sum(value))%>%
        dplyr::filter(sentiment_score != 0) %>% #ignoring the neutral
        dplyr::mutate(sentiment = if_else(sentiment_score > 0, "positive", "negative"))%>%
        dplyr::select(-c(sentiment_score))%>%
        ungroup %>%
        dplyr::mutate(word = "word")%>%
        dplyr::select(ID, word, sentiment, pandem_keyword,country,Period)
      
      
      head(UK_afinn_read)
      
      #compute the probability of observed from the 'absent' group
      #generate the sentiment list based on the prob.
      #assign the 'exist' group with new sentiments based on the prob
      #then plot the expected and observed.
      
      merge_all <- NULL
    
      
      #subset period 
      UK_afinn_read_subset <- UK_afinn_read %>%
        dplyr::filter(Period == periods[y])
      
      length(which(UK_afinn_read_subset$pandem_keyword=="absent"))
      
      #generate expected osbservation  
      
      #compute the observed from the original data
      UK_afinn_OSD <- UK_afinn_read_subset %>% 
        dplyr::select(ID, word, sentiment, pandem_keyword, country)#%>%
        #dplyr::select(-c())
      
      head(UK_afinn_OSD)
      
      #need to generate a new document with 100 x 42 for likert to work
      prob_list <- UK_afinn_OSD %>%
        dplyr::mutate(total=n())%>%
        dplyr::group_by(pandem_keyword)%>%
        dplyr::mutate(len=n())%>%
        ungroup()%>%
        dplyr::group_by(pandem_keyword, sentiment) %>%
        dplyr::mutate(count=n())%>%
        dplyr::mutate(prob_ = count/len) %>% #prob of policing sentiments
        dplyr::distinct(sentiment, pandem_keyword, .keep_all = TRUE)%>%
        dplyr::mutate(share=round((count/total)*100, digits=0))%>%
        dplyr::select(sentiment, pandem_keyword, share)
      

      #now generate the dataframe based on the probilities
      Type_1_Positive_p <- prob_list %>%
        dplyr::filter(sentiment == "positive" & pandem_keyword == "absent")
      Type_1_Positive <- rep("Type_1_Positive", Type_1_Positive_p$share)
      
      Type_1_Negative_p <- prob_list %>%
        dplyr::filter(sentiment == "negative" & pandem_keyword == "absent")
      Type_1_Negative <- rep("Type_1_Negative", Type_1_Negative_p$share)
      
      Type_2_Positive_p <- prob_list %>%
        dplyr::filter(sentiment == "positive" & pandem_keyword == "exist")
      Type_2_Positive <- rep("Type_2_Positive", Type_2_Positive_p$share)
      
      Type_2_Negative_p <- prob_list %>%
        dplyr::filter(sentiment == "negative" & pandem_keyword == "exist")
      Type_2_Negative <- rep("Type_2_Negative", Type_2_Negative_p$share)
      
      #combine all
      merge_all <- c(Type_1_Positive, Type_1_Negative, Type_2_Positive, Type_2_Negative)
      
      
      likert_sentimentData <- cbind(likert_sentimentData, merge_all)
      
      flush.console()
      print(y)

    } 
  }
  
  colnames(likert_sentimentData) <- Pf_names_regions$Police.Force
  write.table(likert_sentimentData, file = paste("/outputs/Likert_Data", periods[y], ".csv", sep="_"), sep=",")
  
  flush.console()
  print(y)
}  
#----------------------------------------------------------
#plot

PR <- c("Period1", "Period2", "Period3")

for(m in 1:length(PR)){
lik_p1 <- read.table(file = paste("/outputs/Likert_Data",PR[m], ".csv", sep="_"), sep=",", head=TRUE)

lik_p1 <- data.frame(lik_p1) %>% mutate_if(is.character,as.factor)

# Make list of ordered factor variables
out <- lapply(lik_p1, function(x) ordered(x, levels = c("Type_1_Positive", "Type_1_Negative", "Type_2_Negative", "Type_2_Positive") ) )

#  Combine into data.frame
res <- do.call( data.frame , out )


# Build plot
p <- likert(res) 

if(m == 1){
  title = "Time Step 1"
    PR1 <- plot(p, centered=FALSE) + ggtitle(title)
  }

if(m == 2){
  title = "Time Step 2"
  PR2 <- plot(p, centered=FALSE) + ggtitle(title)
}

if(m == 3){
  title = "Time Step 3"
  PR3 <- plot(p, centered=FALSE) + ggtitle(title)
}


}
```

```{r, eval=FALSE}
ggarrange(
  PR1,
  common.legend = TRUE, nrow=1, ncol=1, legend = "bottom"
)
```

```{r figs5, echo=FALSE, eval=TRUE, include=TRUE, fig.cap=fig$cap("figs1", "Proportion of tweet types and sentiments per PFA at time step 1"), out.width = '60%', fig.align="center"} 
knitr::include_graphics("figures/likert_T1.png")
```

```{r, eval=FALSE}
ggarrange(
  PR2,
  common.legend = TRUE, nrow=1, ncol=1, legend = "bottom"
)
```

```{r figs6, echo=FALSE, eval=TRUE, include=TRUE, fig.cap=fig$cap("figs1", "Proportion of tweet types and sentiments per PFA at time step 2"), out.width = '60%', fig.align="center"} 
knitr::include_graphics("figures/likert_T2.png")
```

```{r, eval=FALSE}
ggarrange(
  PR3,
  common.legend = TRUE, nrow=1, ncol=1, legend = "bottom"
)
```

```{r figs7, echo=FALSE, eval=TRUE, include=TRUE, fig.cap=fig$cap("figs1", "Proportion of tweet types and sentiments per PFA at time step 3"), out.width = '60%', fig.align="center"} 
knitr::include_graphics("figures/likert_T3.png")
```

# Spatial Maps 

To show the spatial distribution of significant testing results

### Preparing the data 

This section prepares the data in the format necessary for spatial plots 

```{r, eval=FALSE}

#read in p-values files for each time step
sent_pvalues_1 <- read.table(file="/outputs/all_pvalue_Sentiment_ts_1.csv", sep=",", head=TRUE)
sent_pvalues_2 <- read.table(file="/outputs/all_pvalue_Sentiment_ts_2.csv", sep=",", head=TRUE)
sent_pvalues_3 <- read.table(file="/outputs/all_pvalue_Sentiment_ts_3.csv", sep=",", head=TRUE)

#merge data
sent_pvalues_1_2_3 <- data.frame(cbind(sent_pvalues_1$pvalue_sentiment, 
                                       sent_pvalues_2$pvalue_sentiment, 
                                       sent_pvalues_3))

sent_pvalues_1_2_3 <- sent_pvalues_1_2_3 %>%
  dplyr::rename(Period1=sent_pvalues_1.pvalue_sentiment, 
                Period2=sent_pvalues_2.pvalue_sentiment, 
                Period3=pvalue_sentiment,
                PFA=X, Region = X.1)%>%
  dplyr::relocate(Region, .before=Period1) %>%
  dplyr::relocate(PFA, .before=Period1)%>%
  dplyr::select(-c(Region))


sent_pvalues_1_2_3 <- sent_pvalues_1_2_3 %>%
  dplyr::rename(p.Period1=Period1, p.Period2=Period2, p.Period3=Period3)

#read `Position` files
sign_sentm_1 <- read.table(file="/outputs/all_sentiment_signs_ts_1.csv", sep=",", head=TRUE)
sign_sentm_2 <- read.table(file="/outputs/all_sentiment_signs_ts_2.csv", sep=",", head=TRUE)
sign_sentm_3 <- read.table(file="/outputs/all_sentiment_signs_ts_3.csv", sep=",", head=TRUE)

sign_sentm_1_2_3 <- data.frame(cbind(sign_sentm_1$V1, sign_sentm_2$V1, sign_sentm_3))

sign_sentm_1_2_3 <- sign_sentm_1_2_3 %>%
  dplyr::rename(Period1=sign_sentm_1.V1, 
                Period2=sign_sentm_2.V1, 
                Period3=V1,
                PFA=V2, Region = V3)%>%
  dplyr::relocate(Region, .before=Period1) %>%
  dplyr::relocate(PFA, .before=Period1)%>%
  dplyr::select(-c(Region))

sign_sentm_1_2_3 <- sign_sentm_1_2_3 %>%
  dplyr::rename(s.Period1=Period1, s.Period2=Period2, s.Period3=Period3)
#read in the Observations (measure opinions)
sent_scores_1_2_3 <- read.table(file="/outputs/all_obs_exp_Sentiment_ts_1_2_3_ALL.csv", sep=",", head=TRUE)

sent_scores_1_2_3 <- sent_scores_1_2_3 %>%
  dplyr::select(Country, Period, net_sentiment_OSD)

sent_scores_1_2_3 <- t(dcast(sent_scores_1_2_3, Period ~ Country))

sent_scores_1_2_3 <- sent_scores_1_2_3[2:nrow(sent_scores_1_2_3), ]

sent_scores_1_2_3 <- data.frame(cbind(rownames(sent_scores_1_2_3), sent_scores_1_2_3))

colnames(sent_scores_1_2_3) <- c("PFA", "Period1","Period2","Period3")

sent_scores_1_2_3$Period1 <- as.numeric(sent_scores_1_2_3$Period1)
sent_scores_1_2_3$Period2 <- as.numeric(sent_scores_1_2_3$Period2)
sent_scores_1_2_3$Period3 <- as.numeric(sent_scores_1_2_3$Period3)

sent_scores_1_2_3 <- sent_scores_1_2_3 %>%
  dplyr::rename(r.Period1=Period1, r.Period2=Period2, r.Period3=Period3)

PFA <- gsub(".", " ", sent_scores_1_2_3$PFA, fixed=TRUE)

sent_scores_1_2_3$PFA <- PFA

sent_pvalues_sign_1_2_3 <- sent_pvalues_1_2_3 %>%
  dplyr::left_join(sign_sentm_1_2_3)

PFA <- gsub("-", " ", sent_pvalues_sign_1_2_3$PFA, fixed=TRUE)
sent_pvalues_sign_1_2_3$PFA <- PFA



sent_pvalues_sign_scores_1_2_3 <- sent_pvalues_sign_1_2_3 %>%
  dplyr::left_join(sent_scores_1_2_3)

p_class <- NULL

loop_col <- (ncol(sent_pvalues_sign_scores_1_2_3)-1)/3

for(i in 1:loop_col){ #i<-3
  #to assign -ve or +ve depending on the if obs > exp.
  n_ <- data.frame(sent_pvalues_sign_scores_1_2_3[,c(1,(1+i))]) %>% 
    dplyr::rename(p_values = 2)%>%
    dplyr::mutate(p_ = case_when(
      between(p_values, 0, 0.001) ~ "3",
      between(p_values, 0.002, 0.0249) ~ "2",
      between(p_values, 0.0250, 1) ~ "1",
      TRUE ~ NA_character_
    ))%>%
    dplyr::select(c(PFA, p_)) %>%
    dplyr::left_join(data.frame(sent_pvalues_sign_scores_1_2_3[,c(1,(loop_col+1+i))])) %>%
    dplyr::mutate(p_2 = if_else(.[3]=="TRUE", paste(p_,"", sep=""), paste("-",p_,sep="")))%>%
    dplyr::mutate(p_2 = as.numeric(p_2)) %>%
    dplyr::mutate(p_3 = if_else((p_2 == -1 | p_2 == 1), 0, p_2)) 
  
  p_class <- cbind(p_class, n_[,5])
}

colnames(p_class) <- colnames(sent_pvalues_sign_scores_1_2_3)[2: (loop_col+1)] #just borrowing the 'colnames' here.
scores <- sent_pvalues_sign_scores_1_2_3[,c(8:10)]
colnames(scores) <- colnames(sent_pvalues_sign_scores_1_2_3)[5: 7] # also borrowing

final_p_class_scores <- data.frame(cbind(PFA=sent_pvalues_sign_scores_1_2_3[,c(1)], p_class, scores))

#preview file
final_p_class_scores %>% head()

```

## Plotting

```{r, eval=FALSE}

#load shapfiles

#PFA boundaries
study_area <- readOGR(dsn="data", layer="england_wales_pfas") 

#regional boundaries
regional_boundary <- readOGR(dsn=".", layer="regional_boundary") 

regional_boundary <- st_as_sf(regional_boundary)

#plot each result
for(j in 1:3){ #j<-1
  
  study_area_ <- st_as_sf(study_area)
  study_area_ <- study_area_ %>%
    dplyr::left_join(final_p_class_scores)
  
  #choose a point on the surface of each geometry
  #nc3_points <- sf::st_point_on_surface(nc3)
  study_area_points <- sf::st_point_on_surface(study_area_)
  
  # retrieve the coordinates
  study_area_points_coords <- as.data.frame(sf::st_coordinates(study_area_points))
  
  study_area_$to_Fill <- as.data.frame(study_area_points[,(19+j)])[,1]
  #study_area_$to_Fill2 <- as.data.frame(study_area_points[,(19+j)])[,1]
  study_area_points_coords$to_Label <- round(as.data.frame(study_area_points[(22+j)])[,1], digits = 1)
  
  pvalue_list <- c("0.001 {Obs.<<Exp.}", "0.002-0.025 {Obs.<<Exp.}", 
                   "0.026-1.000 {Non Signif.}", "0.002-0.025 {Obs.>>Exp.}", "0.001 {Obs.>>Exp.}")
  p_model <- c(-3, -2, 0, 2, 3)
  
  uni_col <- unique(study_area_$to_Fill)[order(unique(study_area_$to_Fill))]
  
  id_ <- which(!p_model %in% uni_col)
  #p_model <- p_model[-id]
  
  colors <- brewer.pal(n = 5, name = "RdBu")
  
  if(length(id_)!=0){
    p_model <- p_model[-id_]
    colors <- colors[-id_]
    pvalue_list <- pvalue_list[-id_]
  }

  id <- which(uni_col==0)

  ##study_area_$pvalue_list <- 
  regional_ss <- st_centroid(regional_boundary)
  #ggplot()+ 
  
  #dev.new()
  if(j == 1){
    Period1 = ggplot() +
      geom_sf(data = study_area_, aes(fill = as.factor(to_Fill))) +
      ###scale_fill_manual(values = colors) + #,
      #guide = guide_legend(override.aes = list(shape = rep(20, length(pvalue_list)), color = colors, size = 8))) +
      geom_text(data = study_area_points_coords, aes(X, Y, label = to_Label), colour = "white") +
      labs(title = "Period1")+
      labs(fill="P-Values") +
      geom_sf(data = regional_boundary, alpha = 0, cex=1.2, color="black") +
      geom_text(data=regional_ss,aes(x=long,y=lat,label=Regions), 
                nudge_x = 0, nudge_y = 0, 
                check_overlap = F, size=3)+
      scale_fill_manual(values=colors, 
                        labels = pvalue_list)+
      # scale_color_discrete(name="P-Values",
      #                      labels=pvalue_list)+
      #scale_color_hue(labels = pvalue_list)+
      theme_light() #+ 
    #theme_bw() 
  }
  
  if(j == 2){
    Period2 = ggplot() +
      geom_sf(data = study_area_, aes(fill = as.factor(to_Fill))) +
      ##scale_fill_manual(values = colors) + #,
      #guide = guide_legend(override.aes = list(shape = rep(20, length(pvalue_list)), color = colors, size = 8))) +
      geom_text(data = study_area_points_coords, aes(X, Y, label = to_Label), colour = "white") +
      labs(title = "Period2")+
      labs(fill="P-Values") +
      geom_sf(data = regional_boundary, alpha = 0, cex=1.2, color="black") +
      geom_text(data=regional_ss,aes(x=long,y=lat,label=Regions), 
                nudge_x = 0, nudge_y = 0, 
                check_overlap = F, size=3)+
      scale_fill_manual(values=colors, 
                        labels = pvalue_list)+
      #scale_color_hue(labels = pvalue_list)+
      theme_light() #+ 
    #theme_bw() 
  }
  
  if(j == 3){
    Period3 = ggplot() +
      geom_sf(data = study_area_, aes(fill = as.factor(to_Fill))) +
      ##scale_fill_manual(values = colors) + #,
      #guide = guide_legend(override.aes = list(shape = rep(20, length(pvalue_list)), color = colors, size = 8))) +
      geom_text(data = study_area_points_coords, aes(X, Y, label = to_Label), colour = "white") +
      labs(title = "Period3")+
      labs(fill="P-Values") +
      geom_sf(data = regional_boundary, alpha = 0, cex=1.2, color="black") +
      geom_text(data=regional_ss,aes(x=long,y=lat,label=Regions), 
                nudge_x = 0, nudge_y = 0, 
                check_overlap = T, size=3)+
      scale_fill_manual(values=colors, 
                        labels = pvalue_list)+
      #scale_color_hue(labels = pvalue_list)+
      theme_light() #+ 
    #theme_bw() 
  }
  
  
  flush.console()
  print(j)
  
  #sleep()
}
```

```{r, eval=FALSE}
ggarrange(
  Period1,
  common.legend = TRUE, nrow=1, ncol=1, legend = "bottom"
)
```

```{r figs8, echo=FALSE, eval=TRUE, include=TRUE, fig.cap=fig$cap("figs1", "Spatial representation of opinion significance at time step 1"), out.width = '60%', fig.align="center"} 
knitr::include_graphics("figures/map_1.png")
```

```{r, eval=FALSE}
ggarrange(
 Period2,
  common.legend = TRUE, nrow=1, ncol=1, legend = "bottom"
)
```

```{r figs9, echo=FALSE, eval=TRUE, include=TRUE, fig.cap=fig$cap("figs1", "Spatial representation of opinion significance at time step 1"), out.width = '60%', fig.align="center"} 
knitr::include_graphics("figures/map_2.png")
```

```{r, eval=FALSE}
ggarrange(
  Period3,
  common.legend = TRUE, nrow=1, ncol=1, legend = "bottom"
)
```

```{r figs10, echo=FALSE, eval=TRUE, include=TRUE, fig.cap=fig$cap("figs1", "Spatial representation of opinion significance at time step 1"), out.width = '60%', fig.align="center"} 
knitr::include_graphics("figures/map_3.png")
```

